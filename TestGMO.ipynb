{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- augmenter la RAM utilisée par SPARK !! : http://www.datastax.com/dev/blog/zen-art-spark-maintenance, http://www.datastax.com/dev/blog/common-spark-troubleshooting\n",
    "- faire la table dans cassandra\n",
    "- dropper le poids\n",
    "- ajouter une colonne date (full timestamp)\n",
    "- insérer donc fichier par fichier\n",
    "- possible de paralleliser ?\n",
    "- **faire un test sur la vitesse de chargement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x7fbe9e930790>\n"
     ]
    }
   ],
   "source": [
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'spark://172.31.20.22:7077'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1, 2, 3]).map(lambda x: x*2).filter(lambda x: x > 2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 6]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "from time import time\n",
    "from pyspark.sql import SQLContext\n",
    "sql = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On récupère la liste des fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il existe 2160 fichiers.\n"
     ]
    }
   ],
   "source": [
    "directory = '/mnt/wikidata/wikistats/'\n",
    "datafiles = [f for f in listdir(directory) \n",
    "             if isfile(join(directory, f)) and f.startswith('pagecounts')]\n",
    "print('Il existe {} fichiers.'.format(len(datafiles)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A chaque fichier on associer un datastamp :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_date(datafile):\n",
    "    rex = re.compile(r'pagecounts-(\\d{4})(\\d{2})(\\d{2})-(\\d{2}).*')\n",
    "    s = rex.match(datafile)\n",
    "    datetime = '{}-{}-{} {}:00'.format(*(s.group(i) for i in range(1, 5)))\n",
    "    return datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datafiles_datetimes = [(datafile, extract_date(datafile)) for datafile in datafiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pagecounts-20110130-070000.gz', '2011-01-30 07:00'),\n",
       " ('pagecounts-20110305-210000.gz', '2011-03-05 21:00'),\n",
       " ('pagecounts-20110113-180000.gz', '2011-01-13 18:00'),\n",
       " ('pagecounts-20110313-220000.gz', '2011-03-13 22:00'),\n",
       " ('pagecounts-20110101-060000.gz', '2011-01-01 06:00'),\n",
       " ('pagecounts-20110314-110000.gz', '2011-03-14 11:00'),\n",
       " ('pagecounts-20110124-110000.gz', '2011-01-24 11:00'),\n",
       " ('pagecounts-20110116-220000.gz', '2011-01-16 22:00'),\n",
       " ('pagecounts-20110108-230000.gz', '2011-01-08 23:00'),\n",
       " ('pagecounts-20110109-060000.gz', '2011-01-09 06:00')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datafiles_datetimes[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On teste le premier fichier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datafile = datafiles_datetimes[0][0]\n",
    "datetime = datafiles_datetimes[0][1]\n",
    "path='file:///mnt/wikidata/wikistats/' + datafile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(path).map(lambda r: [datetime] + r.split(' '))\n",
    "df = sql.createDataFrame(rdd, schema=['datetime', 'projectcode', 'page', 'views', 'weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.write.format(\"org.apache.spark.sql.cassandra\").\\\n",
    "    options(table=\"wikipediadata\", keyspace=\"projet\").\\\n",
    "    save(mode=\"append\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On time l'insertion de 10 fichiers via le master:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ts = [time()]\n",
    "for datafile, datetime in datafiles_datetimes[1:11]:\n",
    "    rdd = sc.textFile(path).map(lambda r: [datetime] + r.split(' '))\n",
    "    df = sql.createDataFrame(rdd, schema=['datetime', 'projectcode', 'page', 'views', 'weight'])\n",
    "    df.write.format(\"org.apache.spark.sql.cassandra\").\\\n",
    "        options(table=\"wikipediadata\", keyspace=\"projet\").\\\n",
    "        save(mode=\"append\")\n",
    "    ts.append(time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, t in enumerate(ts[1:]):\n",
    "    print('Chargement du fichier {} : {} secondes.'.format(i, t-ts[i-1]))\n",
    "total = ts[-1] - ts[0]\n",
    "m = total // 60\n",
    "s = total % 60\n",
    "print('Temps total pour 10 fichiers : {} minutes {} secondes'.format(m, s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user = sql.read.format(\"org.apache.spark.sql.cassandra\").\\\n",
    "               load(keyspace=\"training\", table=\"user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adults = user.where(user.age > 21)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------------------+-------+\n",
      "|user_id|age|      favorite_foods|   name|\n",
      "+-------+---+--------------------+-------+\n",
      "|      3|108|ArrayBuffer(Muffi...|Patrick|\n",
      "|      1| 34|ArrayBuffer(Bacon...|    Jon|\n",
      "|      2| 22|ArrayBuffer(Kale,...|   Dani|\n",
      "+-------+---+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adults.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
