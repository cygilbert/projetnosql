{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- augmenter la RAM utilisée par SPARK !! : http://www.datastax.com/dev/blog/zen-art-spark-maintenance, http://www.datastax.com/dev/blog/common-spark-troubleshooting : **plus ou moins fait... à ajuster éventuellement**\n",
    "- faire la table dans cassandra **fait : vérifier que le schéma est correct**\n",
    "- dropper le poids ** oublié... à faire : changer le schéma de la table dans cqlsh et changer le code de chargement des DataFrame**\n",
    "- ajouter une colonne date (full timestamp) ** fait **\n",
    "- insérer fichier par fichier ? charger par batch de fichiers ? ** en test **\n",
    "- possible de paralleliser ? ** je ne pense pas : idée ? **\n",
    "- faire un test sur la vitesse de chargement : ** en cours **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vérification du bon fonctionnement de PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x7fbfd0e14910>\n"
     ]
    }
   ],
   "source": [
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'spark://172.31.20.22:7077'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1, 2, 3]).map(lambda x: x*2).filter(lambda x: x > 2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 6]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "from time import time\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions as F\n",
    "sql = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On récupère la liste des fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il existe 2160 fichiers.\n"
     ]
    }
   ],
   "source": [
    "directory = '/mnt/wikidata/wikistats/'\n",
    "datafiles = [f for f in listdir(directory) \n",
    "             if isfile(join(directory, f)) and f.startswith('pagecounts')]\n",
    "print('Il existe {} fichiers.'.format(len(datafiles)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A chaque fichier on associer un datastamp :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_date(datafile):\n",
    "    rex = re.compile(r'pagecounts-(\\d{4})(\\d{2})(\\d{2})-(\\d{2}).*')\n",
    "    s = rex.match(datafile)\n",
    "    datetime = '{}-{}-{} {}:00'.format(*(s.group(i) for i in range(1, 5)))\n",
    "    return datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datafiles_datetimes = [(datafile, extract_date(datafile)) for datafile in datafiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pagecounts-20110130-070000.gz', '2011-01-30 07:00'),\n",
       " ('pagecounts-20110305-210000.gz', '2011-03-05 21:00'),\n",
       " ('pagecounts-20110113-180000.gz', '2011-01-13 18:00'),\n",
       " ('pagecounts-20110313-220000.gz', '2011-03-13 22:00'),\n",
       " ('pagecounts-20110101-060000.gz', '2011-01-01 06:00'),\n",
       " ('pagecounts-20110314-110000.gz', '2011-03-14 11:00'),\n",
       " ('pagecounts-20110124-110000.gz', '2011-01-24 11:00'),\n",
       " ('pagecounts-20110116-220000.gz', '2011-01-16 22:00'),\n",
       " ('pagecounts-20110108-230000.gz', '2011-01-08 23:00'),\n",
       " ('pagecounts-20110109-060000.gz', '2011-01-09 06:00')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datafiles_datetimes[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On teste le premier fichier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datafile = datafiles_datetimes[2][0]\n",
    "datetime = datafiles_datetimes[2][1]\n",
    "path='file:///mnt/wikidata/wikistats/' + datafile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pagecounts-20110101-060000.gz'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datafile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_row(row, datetime):\n",
    "    projectcode, page, views, _ = row.split(' ')\n",
    "    return [datetime, projectcode, page, int(views)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def no_points(row):\n",
    "    return '.' not in row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(path).map(lambda r: create_row(r, datetime)).filter(no_points)\n",
    "df = sql.createDataFrame(rdd, schema=['datetime', 'projectcode', 'page', 'views'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "df.write.format(\"org.apache.spark.sql.cassandra\").\\\n",
    "        options(table=\"wikidata\", keyspace=\"projet\").\\\n",
    "        save(mode=\"append\")\n",
    "t1 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.write.format(\"org.apache.spark.sql.cassandra\").\\\n",
    "    options(table=\"wikipediadata\", keyspace=\"projet\").\\\n",
    "    save(mode=\"append\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On time l'insertion de 10 fichiers via le master:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ts = [time()]\n",
    "for datafile, datetime in datafiles_datetimes[1:11]:\n",
    "    rdd = sc.textFile(path).map(lambda r: [datetime] + r.split(' '))\n",
    "    df = sql.createDataFrame(rdd, schema=['datetime', 'projectcode', 'page', 'views', 'weight'])\n",
    "    df.write.format(\"org.apache.spark.sql.cassandra\").\\\n",
    "        options(table=\"wikipediadata\", keyspace=\"projet\").\\\n",
    "        save(mode=\"append\")\n",
    "    ts.append(time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"log_time\", \"a\") as f:\n",
    "    for i, t in enumerate(ts[1:]):\n",
    "        f.write('Chargement du fichier {} : {} secondes.\\n'.format(i, t-ts[i]))\n",
    "    total = ts[-1] - ts[0]\n",
    "    m = total // 60\n",
    "    s = total % 60\n",
    "    f.write('Temps total pour 10 fichiers : {} minutes {} secondes\\n'.format(m, s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On time l'insertion de 3 fichiers via le master, avec concatenation de ces fichiers avant l'écriture dans Cassandra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On filtre les pages puis on insert dans une df. Et on concatene les df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts = [time()]\n",
    "global_df = None\n",
    "for datafile, datetime in datafiles_datetimes[3:5]:\n",
    "    path='file:///mnt/wikidata/wikistats/' + datafile \n",
    "    rdd = sc.textFile(path).map(lambda r: create_row(r, datetime)).filter(no_points)\n",
    "    df = sql.createDataFrame(rdd, schema=['datetime', 'projectcode', 'page', 'views', 'weight'])\n",
    "    if global_df is None:\n",
    "        global_df = df\n",
    "    else:\n",
    "        global_df = global_df.unionAll(df)\n",
    "    ts.append(time())\n",
    "global_df.write.format(\"org.apache.spark.sql.cassandra\").\\\n",
    "    options(table=\"wikidata\", keyspace=\"projet\").\\\n",
    "    save(mode=\"append\")\n",
    "ts.append(time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"log_time\", \"a\") as f:\n",
    "    for i, t in enumerate(ts[1:]):\n",
    "        f.write('Chargement du fichier {} : {} secondes.\\n'.format(i, t-ts[i]))\n",
    "    f.write('Ecriture de la dataframe de 10 fichiers {} : {} secondes.'.format(i, ts[-1]-ts[-2]))\n",
    "    total = ts[-1] - ts[0]\n",
    "    m = total // 60\n",
    "    s = total % 60\n",
    "    f.write('Temps total pour 10 fichiers : {} minutes {} secondes\\n'.format(m, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15097106"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user = sql.read.format(\"org.apache.spark.sql.cassandra\").\\\n",
    "               load(keyspace=\"training\", table=\"user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adults = user.where(user.age > 21)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------------------+-------+\n",
      "|user_id|age|      favorite_foods|   name|\n",
      "+-------+---+--------------------+-------+\n",
      "|      3|108|ArrayBuffer(Muffi...|Patrick|\n",
      "|      1| 34|ArrayBuffer(Bacon...|    Jon|\n",
      "|      2| 22|ArrayBuffer(Kale,...|   Dani|\n",
      "+-------+---+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adults.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wikidata = sql.read.format(\"org.apache.spark.sql.cassandra\").\\\n",
    "               load(keyspace=\"projet\", table=\"wikidata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datetime: timestamp (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- projectcode: string (nullable = true)\n",
      " |-- views: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wikidata.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|            datetime|  count|\n",
      "+--------------------+-------+\n",
      "|2011-01-01 06:00:...|3390237|\n",
      "|2011-03-13 22:00:...|5706698|\n",
      "|2011-01-30 07:00:...|4245088|\n",
      "|2011-01-13 18:00:...|5901278|\n",
      "|2011-03-05 21:00:...|5816846|\n",
      "+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wikidata.groupBy(\"datetime\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "date = wikidata.filter(wikidata.datetime > '2011-03-05')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------+-----+\n",
      "|            datetime|                page|projectcode|views|\n",
      "+--------------------+--------------------+-----------+-----+\n",
      "|2011-03-05 21:00:...|                   !|         de|    4|\n",
      "|2011-03-05 21:00:...|                   !|         en|   13|\n",
      "|2011-03-05 21:00:...|                   !|         fr|    1|\n",
      "|2011-03-05 21:00:...|                   !|         pl|    1|\n",
      "|2011-03-05 21:00:...|                   !|         ro|    1|\n",
      "|2011-03-05 21:00:...|                   !|         ru|    1|\n",
      "|2011-03-05 21:00:...|                  !!|         de|    3|\n",
      "|2011-03-05 21:00:...|                  !!|         en|    3|\n",
      "|2011-03-05 21:00:...|                  !!|         fr|    2|\n",
      "|2011-03-05 21:00:...|                  !!|         zh|    1|\n",
      "|2011-03-05 21:00:...|                 !!!|         de|    5|\n",
      "|2011-03-05 21:00:...|                 !!!|         en|   21|\n",
      "|2011-03-05 21:00:...|                 !!!|         fr|    2|\n",
      "|2011-03-05 21:00:...|                 !!!|         it|    1|\n",
      "|2011-03-05 21:00:...|      !!!Fuck_You!!!|         en|    6|\n",
      "|2011-03-05 21:00:...|!!!Fuck_You!!!_An...|         en|    1|\n",
      "|2011-03-05 21:00:...|!!!Fuck_You!!!_an...|         en|    1|\n",
      "|2011-03-05 21:00:...|!!!Fuck_You!!!_an...|         pt|    1|\n",
      "|2011-03-05 21:00:...|  !!Destroy-Oh-Boy!!|         en|    1|\n",
      "|2011-03-05 21:00:...|!%20Videos2Flash%...|        www|    1|\n",
      "+--------------------+--------------------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dategroup = date.groupBy(\"datetime\", \"page\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job 190 cancelled because Stage 250 was cancelled\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1276)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1228)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply$mcVI$sp(DAGScheduler.scala:1216)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply(DAGScheduler.scala:1215)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply(DAGScheduler.scala:1215)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:156)\n\tat org.apache.spark.scheduler.DAGScheduler.handleStageCancellation(DAGScheduler.scala:1215)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1433)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1421)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-161-9d613924f431>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdategroup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviews\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/share/dse/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    279\u001b[0m         \"\"\"\n\u001b[0;32m    280\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 281\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjavaToPython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    282\u001b[0m         \u001b[0mrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_create_cls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/share/dse/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 538\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/share/dse/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job 190 cancelled because Stage 250 was cancelled\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1276)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1228)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply$mcVI$sp(DAGScheduler.scala:1216)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply(DAGScheduler.scala:1215)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply(DAGScheduler.scala:1215)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:156)\n\tat org.apache.spark.scheduler.DAGScheduler.handleStageCancellation(DAGScheduler.scala:1215)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1433)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1421)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n"
     ]
    }
   ],
   "source": [
    "res = dategroup.agg(F.sum(date.views)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pagecounts-20110101-060000.gz', '2011-01-01 06:00'),\n",
       " ('pagecounts-20110314-110000.gz', '2011-03-14 11:00')]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datafiles_datetimes[4:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# val results = sql.sql(\"SELECT page FROM wikidata where\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_pages_period(date_from, date_to, table):\n",
    "    result = table.filter((table.datetime > date_from) & (table.datetime < date_to))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = get_pages_period('2011-03-01', '2011-03-30', wikidata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_datetime = test.groupBy(\"page\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|datetime|count|\n",
      "+--------+-----+\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_datetime.count().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
